{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP Text Preprocessing.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPOdldKZmuyfozYmUjoO6XD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ankit4371/Machine-Learning-Notebooks/blob/main/NLP/NLP_Text_Preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHN9BWZNyhrY"
      },
      "source": [
        "# NLP\r\n",
        "## Text Preprocessing:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YvQwBQTzUYN"
      },
      "source": [
        "text = \"I believe this would help us to learn Text Prepocessing.\r\n",
        "         It's a simple python code to demonstrate it.\""
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akvxmAjyylKr"
      },
      "source": [
        "### Tokenization  \r\n",
        "Task of breaking of a text into pieces called as tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcyJGX5DzByn"
      },
      "source": [
        "#### Sentence Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42lCfgd-yOQl",
        "outputId": "cdb160db-3fde-4298-f445-759fbdd49ae4"
      },
      "source": [
        "import nltk\r\n",
        "nltk.download('punkt')\r\n",
        "from nltk.tokenize import sent_tokenize\r\n",
        "\r\n",
        "sents = sent_tokenize(text)\r\n",
        "print(sents)\r\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "['I believe this would help us to learn Text Prepocessing.', \"It's a simple python code to demonstrate it.\"]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uO6FkAo0JjF"
      },
      "source": [
        "#### Word Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6Cq2BvFz1WU",
        "outputId": "df661ae0-e7f3-447b-fde3-9fa7c108f501"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\r\n",
        "words = [word_tokenize(sent) for sent in sents]\r\n",
        "print(words)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['I', 'believe', 'this', 'would', 'help', 'us', 'to', 'learn', 'Text', 'Prepocessing', '.'], ['It', \"'s\", 'a', 'simple', 'python', 'code', 'to', 'demonstrate', 'it', '.']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMl-mcbV0bK2",
        "outputId": "f5ce9b5f-2358-4774-9a6c-6ebad0e5814c"
      },
      "source": [
        "# OR\r\n",
        "words = word_tokenize(text)\r\n",
        "print(words)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['I', 'believe', 'this', 'would', 'help', 'us', 'to', 'learn', 'Text', 'Prepocessing', '.', 'It', \"'s\", 'a', 'simple', 'python', 'code', 'to', 'demonstrate', 'it', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RyMU2oZo0u2w"
      },
      "source": [
        "### Stop Word Removal\r\n",
        "English Words which does not add much meaning to a sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uOdQrJTT0kz3",
        "outputId": "51ef6a7a-2bd9-4e80-df2c-c95a270a3594"
      },
      "source": [
        "from nltk.corpus import stopwords\r\n",
        "nltk.download('stopwords')\r\n",
        "from string import punctuation\r\n",
        "print(punctuation)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cVjuNMZM3KeA",
        "outputId": "09de0a89-571d-4d0a-c52a-603fb86255b9"
      },
      "source": [
        "set(stopwords.words('english'))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'a',\n",
              " 'about',\n",
              " 'above',\n",
              " 'after',\n",
              " 'again',\n",
              " 'against',\n",
              " 'ain',\n",
              " 'all',\n",
              " 'am',\n",
              " 'an',\n",
              " 'and',\n",
              " 'any',\n",
              " 'are',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'as',\n",
              " 'at',\n",
              " 'be',\n",
              " 'because',\n",
              " 'been',\n",
              " 'before',\n",
              " 'being',\n",
              " 'below',\n",
              " 'between',\n",
              " 'both',\n",
              " 'but',\n",
              " 'by',\n",
              " 'can',\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'd',\n",
              " 'did',\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'do',\n",
              " 'does',\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'doing',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'down',\n",
              " 'during',\n",
              " 'each',\n",
              " 'few',\n",
              " 'for',\n",
              " 'from',\n",
              " 'further',\n",
              " 'had',\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'has',\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'have',\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'having',\n",
              " 'he',\n",
              " 'her',\n",
              " 'here',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'him',\n",
              " 'himself',\n",
              " 'his',\n",
              " 'how',\n",
              " 'i',\n",
              " 'if',\n",
              " 'in',\n",
              " 'into',\n",
              " 'is',\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'it',\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " 'just',\n",
              " 'll',\n",
              " 'm',\n",
              " 'ma',\n",
              " 'me',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'more',\n",
              " 'most',\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'my',\n",
              " 'myself',\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'now',\n",
              " 'o',\n",
              " 'of',\n",
              " 'off',\n",
              " 'on',\n",
              " 'once',\n",
              " 'only',\n",
              " 'or',\n",
              " 'other',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'out',\n",
              " 'over',\n",
              " 'own',\n",
              " 're',\n",
              " 's',\n",
              " 'same',\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'she',\n",
              " \"she's\",\n",
              " 'should',\n",
              " \"should've\",\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " 'so',\n",
              " 'some',\n",
              " 'such',\n",
              " 't',\n",
              " 'than',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'the',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'them',\n",
              " 'themselves',\n",
              " 'then',\n",
              " 'there',\n",
              " 'these',\n",
              " 'they',\n",
              " 'this',\n",
              " 'those',\n",
              " 'through',\n",
              " 'to',\n",
              " 'too',\n",
              " 'under',\n",
              " 'until',\n",
              " 'up',\n",
              " 've',\n",
              " 'very',\n",
              " 'was',\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'we',\n",
              " 'were',\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " 'what',\n",
              " 'when',\n",
              " 'where',\n",
              " 'which',\n",
              " 'while',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'why',\n",
              " 'will',\n",
              " 'with',\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\",\n",
              " 'y',\n",
              " 'you',\n",
              " \"you'd\",\n",
              " \"you'll\",\n",
              " \"you're\",\n",
              " \"you've\",\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-MuuCo281QgL",
        "outputId": "1dc450cc-e60e-4364-d380-18ad1c81e4b0"
      },
      "source": [
        "custom_list = set(stopwords.words('english')+list(punctuation))\r\n",
        "word_list = [word for word in word_tokenize(text) if word not in custom_list]\r\n",
        "print(word_list)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['I', 'believe', 'would', 'help', 'us', 'learn', 'Text', 'Prepocessing', 'It', \"'s\", 'simple', 'python', 'code', 'demonstrate']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32f9sHlH3U-T"
      },
      "source": [
        "### N-Grams"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWNwNPJr48PJ"
      },
      "source": [
        "An n-gram is a contigous sequence of n-items from a given sample of text or speech."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WuNLGTcF2ItT",
        "outputId": "3ba8d903-6fa1-4602-9a5f-c2e398482c6e"
      },
      "source": [
        "# BiGram\r\n",
        "from nltk.collocations import BigramCollocationFinder\r\n",
        "finder = BigramCollocationFinder.from_words(word_list)\r\n",
        "print(finder.ngram_fd.items())"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_items([(('I', 'believe'), 1), (('believe', 'would'), 1), (('would', 'help'), 1), (('help', 'us'), 1), (('us', 'learn'), 1), (('learn', 'Text'), 1), (('Text', 'Prepocessing'), 1), (('Prepocessing', 'It'), 1), (('It', \"'s\"), 1), ((\"'s\", 'simple'), 1), (('simple', 'python'), 1), (('python', 'code'), 1), (('code', 'demonstrate'), 1)])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bG632YTn32Q6",
        "outputId": "f3c90215-951b-4c4d-b803-1101da77826a"
      },
      "source": [
        "from nltk.collocations import TrigramCollocationFinder\r\n",
        "finder3 = TrigramCollocationFinder.from_words(word_list)\r\n",
        "print(finder3.ngram_fd.items())"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_items([(('I', 'believe', 'would'), 1), (('believe', 'would', 'help'), 1), (('would', 'help', 'us'), 1), (('help', 'us', 'learn'), 1), (('us', 'learn', 'Text'), 1), (('learn', 'Text', 'Prepocessing'), 1), (('Text', 'Prepocessing', 'It'), 1), (('Prepocessing', 'It', \"'s\"), 1), (('It', \"'s\", 'simple'), 1), ((\"'s\", 'simple', 'python'), 1), (('simple', 'python', 'code'), 1), (('python', 'code', 'demonstrate'), 1)])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fmi79OFj5Iha"
      },
      "source": [
        "### Steaming\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JigAIloM5Oet"
      },
      "source": [
        "Process of reducing inflected(or sometimes derived) words to their word stem,base or root form."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kzbOaif64mtP",
        "outputId": "ddc64707-ee73-43c4-aa0e-7a3f628d2282"
      },
      "source": [
        "from nltk.stem.porter import PorterStemmer\r\n",
        "from nltk.stem.lancaster import LancasterStemmer \r\n",
        "L_S = LancasterStemmer ()\r\n",
        "P_S= PorterStemmer()\r\n",
        "new_text = \"It is important to be very pythonly while you are pythoning\\\r\n",
        "            with python. All pythoners have pythoned poorly at least once.\"\r\n",
        "print(\"Porter Steamming\")\r\n",
        "porter_stem = [P_S.stem(word) for word in word_tokenize(new_text)]\r\n",
        "print(porter_stem)\r\n",
        "print(\"Lancaster Steamming\")\r\n",
        "lancaster_stem = [L_S.stem(word) for word in word_tokenize(new_text)]\r\n",
        "print(lancaster_stem)\r\n"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Porter Steamming\n",
            "['It', 'is', 'import', 'to', 'be', 'veri', 'pythonli', 'while', 'you', 'are', 'python', 'with', 'python', '.', 'all', 'python', 'have', 'python', 'poorli', 'at', 'least', 'onc', '.']\n",
            "Lancaster Steamming\n",
            "['it', 'is', 'import', 'to', 'be', 'very', 'python', 'whil', 'you', 'ar', 'python', 'with', 'python', '.', 'al', 'python', 'hav', 'python', 'poor', 'at', 'least', 'ont', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOtrofYj920S"
      },
      "source": [
        "### Word Sense Disambiguation "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sM6zxXVg--oS"
      },
      "source": [
        "Identifying sense(context) of word"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FP6bygyg6F_V",
        "outputId": "89d5dd52-c55c-4f34-b256-ab675b6d63c6"
      },
      "source": [
        "nltk.download('wordnet')"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NOUinE6I-FpB",
        "outputId": "5010500d-025e-4258-d991-c07547a3ebee"
      },
      "source": [
        "from nltk.corpus import wordnet\r\n",
        "for ss in wordnet.synsets(\"mouse\"):\r\n",
        "  print(ss,ss.definition())"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Synset('mouse.n.01') any of numerous small rodents typically resembling diminutive rats having pointed snouts and small ears on elongated bodies with slender usually hairless tails\n",
            "Synset('shiner.n.01') a swollen bruise caused by a blow to the eye\n",
            "Synset('mouse.n.03') person who is quiet or timid\n",
            "Synset('mouse.n.04') a hand-operated electronic device that controls the coordinates of a cursor on your computer screen as you move it around on a pad; on the bottom of the device is a ball that rolls on the surface of the pad\n",
            "Synset('sneak.v.01') to go stealthily or furtively\n",
            "Synset('mouse.v.02') manipulate the mouse of a computer\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VXf-YTM3-XlR",
        "outputId": "277597e5-49c8-475e-9a64-727027d030d9"
      },
      "source": [
        "from nltk.wsd import lesk\r\n",
        "context_1 = lesk(word_tokenize(\"Sing in a lower tone, along with the bass\"),'bass')\r\n",
        "print(context_1,context_1.definition())"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Synset('bass.n.07') the member with the lowest range of a family of musical instruments\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jD5gpW_U_s5u",
        "outputId": "da6199e2-c48b-4b80-a64c-68d4267f97cd"
      },
      "source": [
        "context_2 = lesk(word_tokenize(\"the bass is very hard to catch.\"),'bass')\r\n",
        "print(context_2,context_2.definition())"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Synset('sea_bass.n.01') the lean flesh of a saltwater fish of the family Serranidae\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2umffjJoAjXt"
      },
      "source": [
        "### Count Vectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4f6qcCmVGC51"
      },
      "source": [
        "Provides a simple way to both  \r\n",
        "* tokenize a collection of text\r\n",
        "* build a vocubulary of known words  \r\n",
        "but also to encode a new documents using that vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        },
        "id": "uO2Bsg4VAFCV",
        "outputId": "32b98631-8a5d-4617-ea7a-6cf47b5aa479"
      },
      "source": [
        "import pandas as pd\r\n",
        "corpus = [\r\n",
        "          \"This is the first elment from heaven\", \r\n",
        "          \"but the second document is from mars\",\r\n",
        "          \"And this is the third one from nowhere\",\r\n",
        "          \"Is this the first document from nowhere?\",\r\n",
        "]\r\n",
        "df = pd.DataFrame({'text': corpus})\r\n",
        "df"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>This is the first elment from heaven</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>but the second document is from mars</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>And this is the third one from nowhere</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Is this the first document from nowhere?</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                       text\n",
              "0      This is the first elment from heaven\n",
              "1      but the second document is from mars\n",
              "2    And this is the third one from nowhere\n",
              "3  Is this the first document from nowhere?"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2GRXeO4iA3sL",
        "outputId": "9ffa7a84-02a3-4975-9fae-d2045da652cc"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\r\n",
        "count_vect = CountVectorizer()\r\n",
        "count_vect =CountVectorizer(stop_words=['this','is'])\r\n",
        "X = count_vect.fit_transform(df.text).toarray()\r\n",
        "print(X)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 0 0 1 1 1 1 0 0 0 0 1 0]\n",
            " [0 1 1 0 0 1 0 1 0 0 1 1 0]\n",
            " [1 0 0 0 0 1 0 0 1 1 0 1 1]\n",
            " [0 0 1 0 1 1 0 0 1 0 0 1 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vwlmZ0ljCm0q",
        "outputId": "026ff847-b6a9-497a-fa9a-59d3cdcf879e"
      },
      "source": [
        "print(count_vect.vocabulary_)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'the': 11, 'first': 4, 'elment': 3, 'from': 5, 'heaven': 6, 'but': 1, 'second': 10, 'document': 2, 'mars': 7, 'and': 0, 'third': 12, 'one': 9, 'nowhere': 8}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5LzaxTrDMAY"
      },
      "source": [
        "TF - IDF Vectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNl3P1DYFsoP"
      },
      "source": [
        "TF - IDF ar word frequency scores that try to highlight words that more interesting i.e. more frequent in document."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p6Mk2YEUC1zL",
        "outputId": "03b3f430-fd57-4dff-848f-c6774e779f2c"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
        "tfidf = TfidfVectorizer()\r\n",
        "tfidf.fit(corpus)\r\n",
        "print(tfidf.vocabulary_)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'this': 14, 'is': 7, 'the': 12, 'first': 4, 'elment': 3, 'from': 5, 'heaven': 6, 'but': 1, 'second': 11, 'document': 2, 'mars': 8, 'and': 0, 'third': 13, 'one': 10, 'nowhere': 9}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ojX2dDj7DspU",
        "outputId": "1c9aaac6-df84-416a-d6ae-c32ff0d2bdc7"
      },
      "source": [
        "print(tfidf.idf_)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1.91629073 1.91629073 1.51082562 1.91629073 1.51082562 1.\n",
            " 1.91629073 1.         1.91629073 1.51082562 1.91629073 1.91629073\n",
            " 1.         1.91629073 1.22314355]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4yKxRhSD6cu"
      },
      "source": [
        "### Hashing Vectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p13ItKKXFNeS"
      },
      "source": [
        "Solve the issue of other Vectorizers i.e. large number of counts and frequencies.  \r\n",
        "Hash of words => Integers  \r\n",
        "No vocabulary required  \r\n",
        "Downside: No way to convert encoding integers back to word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vsn-e5bNDyNV",
        "outputId": "86262f27-7415-4dbb-f567-408c9d8968fe"
      },
      "source": [
        "from sklearn.feature_extraction.text import HashingVectorizer\r\n",
        "hash_vect = HashingVectorizer(n_features = 8, norm= None, alternate_sign = False)\r\n",
        "hash_vect.fit_transform(df.text).toarray()"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 1., 0., 1., 1., 1., 2., 0.],\n",
              "       [2., 0., 0., 1., 1., 1., 2., 0.],\n",
              "       [0., 0., 0., 0., 2., 3., 3., 0.],\n",
              "       [2., 0., 0., 0., 1., 1., 3., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76VJ894KExYR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}